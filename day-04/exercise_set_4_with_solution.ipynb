{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-pnHB5q0yBp7"
   },
   "source": [
    "# Scientific Libraries, Monte Carlo, and More Linear Algebra Exercises"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\"><button><a href=\"https://colab.research.google.com/github/QuantEcon/workshop.africa-july2023/blob/main/day-04/exercise_set_4_with_solution.ipynb\"><img src=\"\" heght=\"10px\"/><img\n",
    "    src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
    "    alt=\"open with Colab\" width=\"100px\"/></a></button></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Written for the QuantEcon Africa Workshop (July 2023)\n",
    "#### Author: [Smit Lunagariya](https://github.com/Smit-create), [Humphrey Yang](https://github.com/HumphreyYang) and [Shu Hu](https://shu-hu.com/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains exercises on scientific libraries, Monte Carlo simulation, and eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4DjbqlLyYXB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import eig\n",
    "from numpy.random import randn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Before you attempt these exercises, we recommend that you read\n",
    "\n",
    "1. the [lecture on NumPy](https://python-programming.quantecon.org/numpy.html),\n",
    "2. the [lecture on Matplotlib](https://python-programming.quantecon.org/matplotlib.html) and\n",
    "3. the [lecture on SciPy](https://python-programming.quantecon.org/scipy.html).\n",
    "4. the [lecture on Eigenvalues and Eigenvectrors](https://intro.quantecon.org/eigen_I.html).\n",
    "5. [An introduction to Monte Carlo](https://intro.quantecon.org/monte_carlo.html#an-introduction-to-monte-carlo)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Given a matrix $A$, compute the eigenvalues and eigenvectors and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 4], [5, 7]])\n",
    "A"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals, evecs = eig(A)\n",
    "print(\"eigenvalues:\\n\", evals)\n",
    "print(\"eigenvectors:\\n\", evecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evecs = evecs[:, 0], evecs[:, 1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "# Set the axes through the origin\n",
    "for spine in ['left', 'bottom']:\n",
    "    ax.spines[spine].set_position('zero')\n",
    "for spine in ['right', 'top']:\n",
    "    ax.spines[spine].set_color('none')\n",
    "\n",
    "ax.grid(alpha=0.4)\n",
    "\n",
    "xmin, xmax = -3, 3\n",
    "ymin, ymax = -3, 3\n",
    "ax.set(xlim=(xmin, xmax), ylim=(ymin, ymax))\n",
    "\n",
    "# Plot each eigenvector\n",
    "for v in evecs:\n",
    "    ax.annotate('', xy=v, xytext=(0, 0),\n",
    "                arrowprops=dict(facecolor='blue',\n",
    "                shrink=0,\n",
    "                alpha=0.6,\n",
    "                width=0.5))\n",
    "\n",
    "# Plot the lines they run through\n",
    "x = np.linspace(xmin, xmax, 3)\n",
    "for v in evecs:\n",
    "    a = v[1] / v[0]\n",
    "    ax.plot(x, a * x, 'b-', lw=0.4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "In this exercise, use the following imports to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.1** Draw 1000 independent draws from the standard normal distribution using [`scipy.stats`](https://docs.scipy.org/doc/scipy/reference/stats.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the standard normal distribution\n",
    "mu = 0\n",
    "sigma = 1\n",
    "\n",
    "# Generate a sample of 10000 draws from the normal distribution\n",
    "samples = norm.rvs(mu, sigma, size=10_000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.2** Calculate the sample mean and variance of the draws and compare them to the theoretical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and standard deviation of the sample\n",
    "mean = np.mean(samples)\n",
    "std = np.std(samples)\n",
    "\n",
    "print(\"Sample mean: {:.4f}\".format(mean))\n",
    "print(\"Theoretical mean: {:.4f}\".format(mu))\n",
    "print(\"Sample standard deviation: {:.4f}\".format(std))\n",
    "print(\"Theoretical standard deviation: {:.4f}\".format(sigma))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.3** Visualize the empirical distribution of the draws using a histogram. Mark the sample mean and $\\pm$ one standard deviation using vertical lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram of the sample\n",
    "plt.hist(samples, bins=100)\n",
    "plt.axvline(mean, color='red')\n",
    "plt.axvline(mean + std, color='grey')\n",
    "plt.axvline(mean - std, color='grey')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.4** Calculate the probability that a draw from the distribution is less than 0 and compare it to the proportion of the sample that is less than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the probability that a draw from the distribution is less than 0\n",
    "probability = norm.cdf(0, mu, sigma)\n",
    "\n",
    "print(\"The probability that a draw from the distribution is less than 0 is:\", \n",
    "      probability)\n",
    "\n",
    "# Calculate the proportion of samples is less than 0\n",
    "proportion = np.mean(samples < 0)\n",
    "\n",
    "print(\"The proportion of samples is less than 0 is:\", proportion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.5** Will results in exercise 2.3 change with different \n",
    "sample sizes? \n",
    "\n",
    "Try to draw 10, 100, 500, 1000, and 10000 samples. Repeat 100 times for each sample size and plot the results. What do you observe when comparing the sample estimates to the population parameters (the ground truth)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that draws n samples from \n",
    "# the standard normal distribution and returns \n",
    "# mean and starndard deviation\n",
    "def draw_samples(n):\n",
    "    samples = norm.rvs(mu, sigma, size=n)\n",
    "    mean = np.mean(samples)\n",
    "    std = np.std(samples)\n",
    "    return mean, std\n",
    "\n",
    "sample_sizes = [10, 100, 500, 1000, 10_000]\n",
    "sizes_arr = np.repeat(sample_sizes, 100)\n",
    "\n",
    "# Draw 100 samples for each sample size using vectorized numpy code\n",
    "means, stds = np.vectorize(draw_samples)(sizes_arr)\n",
    "means, stds = means.reshape(5, 100), stds.reshape(5, 100)\n",
    "\n",
    "for i in range(5):\n",
    "    # Plot scatter plot of means and standard deviations\n",
    "    plt.scatter(means[i], stds[i], alpha=0.5, \n",
    "                label=\"n = {}\".format(sample_sizes[i]))\n",
    "    plt.axhline(sigma, color='red')\n",
    "    plt.axvline(mu, color='red')\n",
    "    plt.xlabel(\"Sample mean\")\n",
    "    plt.ylabel(\"Sample standard deviation\")\n",
    "    plt.legend()\n",
    "    # Limit the axis to zoom in on the plot\n",
    "    plt.xlim(-0.5, 0.5)\n",
    "    plt.ylim(0.5, 1.5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Consider a random variable $S$ in the following form\n",
    "\n",
    "$$\n",
    "S = \\left ( \\sum^M_{i=1} X_i \\right )^p\n",
    "$$\n",
    "\n",
    "where $X_i \\sim LN (\\mu_i, \\sigma_i)$, $M\\in \\mathbb N$ and $p$ is a positive number known to us."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.1\n",
    "\n",
    "First let $M=1$ and $p=1$. Now we have\n",
    "\n",
    "$$\n",
    "    S = X_1\n",
    "$$\n",
    "where $X_1 \\sim LN (\\mu_1, \\sigma_1)$.\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "**Exercise 3.1.1**\n",
    "\n",
    "Write down the analytical solutions to $\\mathbb E S$ and $\\mathop{\\mathrm{Var}} S$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution to Exercise 3.1.1**\n",
    "\n",
    "See section [*Share price with known distribution*](https://intro.quantecon.org/monte_carlo.html#share-price-with-known-distribution)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.1.2**\n",
    "\n",
    "Define two Python functions `mean_analytical` and `var_analytical` to compute the $\\mathbb E S$ and $\\mathop{\\mathrm{Var}} S$ directly according to their analytical solutions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution to Exercise 3.1.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_analytical(μ_1, σ_1):\n",
    "    return np.exp(μ_1 + σ_1 ** 2 / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_analytical(μ_1, σ_1):\n",
    "    return (np.exp(σ_1 ** 2) - 1) * np.exp(2 * μ_1 + σ_1 ** 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.1.3**\n",
    "\n",
    "Given "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1_000_000\n",
    "p = 1\n",
    "μ_1 = 0.2\n",
    "σ_1 = 0.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate $\\mathbb E S$ by Monte Carlo simulation using loop. \n",
    "\n",
    "Compare the result with the analytical solution.\n",
    "\n",
    "Hint. \n",
    "$$\n",
    "\\mathbb E S \\approx  \n",
    "\\frac{1}{n} \\sum_{i=1}^n S_i    \n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution to Exercise 3.1.3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 0.0\n",
    "for i in range(n):\n",
    "    X_1 = np.exp(μ_1 + σ_1 * randn())\n",
    "    S += (X_1)**p\n",
    "S_mean_mc = S / n\n",
    "S_mean_analytical = mean_analytical(μ_1, σ_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Monte Carlo simulation of ES in loop is \", S_mean_mc)  \n",
    "print(\"Analytical solution result of ES is \", S_mean_analytical)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.1.4**\n",
    "\n",
    "With the same setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1_000_000\n",
    "p = 1\n",
    "μ_1 = 0.2\n",
    "σ_1 = 0.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and given $\\mathbb ES$, approximate $\\mathop{\\mathrm{Var}} S$ by Monte Carlo simulation.\n",
    "\n",
    "Compare it with the analytical solution.\n",
    "\n",
    "Hint. Note that $\\mathop{\\mathrm{Var}} S = \\mathbb E \\left [S - \\mathbb E S \\right ]^2$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution to Exercise 3.1.4**\n",
    "\n",
    "We can use the result of either the analytical solution or the Monte Carolo simulation for $\\mathbb S$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 0.0\n",
    "for i in range(n):\n",
    "    X_1 = np.exp(μ_1 + σ_1 * randn())\n",
    "    S += ((X_1)**p - S_mean_analytical) ** 2 \n",
    "S_var_mc = S / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_var_analytical = var_analytical(μ_1, σ_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Monte Carlo simulation of VarS in loop is \", S_var_mc)  \n",
    "print(\"Analytical solution result of VarS is \", S_var_analytical)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.1.5**\n",
    "\n",
    "With the same setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1_000_000\n",
    "p = 1\n",
    "μ_1 = 0.2\n",
    "σ_1 = 0.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite the loops in Exercises 1.1.3 and 1.1.4 with vectorization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution to Exercise 3.1.5**\n",
    "\n",
    "First $\\mathbb E S$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = np.exp(μ_1 + σ_1 * randn(n))\n",
    "S = (X_1)**p\n",
    "S_mean_mc = S.mean()\n",
    "print(\"Monte Carlo simulation of ES in vectorization is \", S_mean_mc)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then $\\mathop{\\mathrm{Var}} S$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = np.exp(μ_1 + σ_1 * randn(n))\n",
    "S = ((X_1)**p - S_mean_analytical) ** 2\n",
    "S_var_mc = S.mean()\n",
    "print(\"Monte Carlo simulation of VarS in vectorization is \", S_var_mc) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.2\n",
    "\n",
    "Let $M=3$. Then we have\n",
    "\n",
    "$$\n",
    "    S = (X_1 + X_2 + X_3)^p\n",
    "$$\n",
    "which is the same form as our lecture [here](https://intro.quantecon.org/monte_carlo.html#share-price-with-unknown-distribution).\n",
    "\n",
    "Now we don't have analytical solutions for $\\mathbb E S$ and $\\mathop{\\mathrm{Var}} S$.\n",
    "\n",
    "We use the following values for $ p $ and each $ \\mu_i $ and $ \\sigma_i $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1_000_000\n",
    "p = 0.5\n",
    "μ_1, μ_2, μ_3 = 0.2, 0.8, 0.4\n",
    "σ_1, σ_2, σ_3 = 0.1, 0.05, 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_vectorized(n=1_000_000):\n",
    "    X_1 = np.exp(μ_1 + σ_1 * randn(n))\n",
    "    X_2 = np.exp(μ_2 + σ_2 * randn(n))\n",
    "    X_3 = np.exp(μ_3 + σ_3 * randn(n))\n",
    "    S = (X_1 + X_2 + X_3)**p\n",
    "    return S.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_mean_mc_new = compute_mean_vectorized(n)\n",
    "print(\"Monte Carlo simulation of ES in vectorization is \", S_mean_mc_new)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.2.1** \n",
    "\n",
    "Given the monte carlo simulation of $\\mathbb ES$, approximate $\\mathop{\\mathrm{Var}} S$ by Monte Carlo simulation using loop."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution to Exercise 3.2.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 0.0\n",
    "for i in range(n):\n",
    "    X_1 = np.exp(μ_1 + σ_1 * randn())\n",
    "    X_2 = np.exp(μ_2 + σ_2 * randn())\n",
    "    X_3 = np.exp(μ_3 + σ_3 * randn())\n",
    "    S += ((X_1 + X_2 + X_3)**p - S_mean_mc_new) ** 2\n",
    "S_var_mc_new_loop = S / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Monte Carlo simulation of VarS in loop is\", S_var_mc_new_loop) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.2.2**\n",
    "\n",
    "Rewrite the loop in Exercise 3.2.1 with vectorization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution to Exercise 3.2.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = np.exp(μ_1 + σ_1 * randn(n))\n",
    "X_2 = np.exp(μ_2 + σ_2 * randn(n))\n",
    "X_3 = np.exp(μ_3 + σ_3 * randn(n))\n",
    "S = ((X_1 + X_2 + X_3)**p - S_mean_mc_new) ** 2\n",
    "S_var_mc_new = S.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Monte Carlo simulation of VarS in vectorization is\", S_var_mc_new) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate and plot the correlated time series\n",
    "\n",
    "$$\n",
    "    x_{t+1} = \\alpha \\, x_t + \\epsilon_{t+1}\n",
    "    \\quad \\text{where} \\quad\n",
    "    x_0 = 0 \n",
    "    \\quad \\text{and} \\quad t = 0,\\ldots,T\n",
    "$$\n",
    "\n",
    "Here $\\{\\epsilon_t\\}$ is iid and standard normal.\n",
    "\n",
    "In your solution, restrict your import statements to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import normalvariate\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set $T=200$ and $\\alpha = 0.9$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Set random seed to replicate the solution\n",
    "# (This is optional and used so that each time you run the cell, \n",
    "# you get same results)\n",
    "random.seed(2023)\n",
    "\n",
    "alpha = 0.9\n",
    "ts_length = 200\n",
    "x = 0\n",
    "\n",
    "x_values = []\n",
    "for i in range(ts_length):\n",
    "    x_values.append(x)\n",
    "    x = alpha * x + normalvariate(0, 1)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_values, '-')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On day 2, we generated 100000 data points from the [exponential distribution](https://en.wikipedia.org/wiki/Exponential_distribution) with density\n",
    "\n",
    "$$\n",
    "f(x; \\alpha) = \\alpha \\exp(-\\alpha x)\n",
    "\\qquad\n",
    "(x > 0, \\alpha > 0)\n",
    "$$\n",
    "\n",
    "taking $\\alpha = 0.5$. Then\n",
    "\n",
    "Using the same data set, implement maximum likelihood again, but this time pretending that you don't know the analytical expression for the maximum likelihood estimator.  Instead, set up the log likelihood function and maximize it numerically using a routine from `scipy.optimize`. \n",
    "\n",
    "(Hint: Have a look at the optimization examples from the scientific Python quickstart notebook.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's set up the log likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon\n",
    "\n",
    "alpha = 0.5\n",
    "n = int(1e5)\n",
    "# Scale controls the exponential parameter\n",
    "ep = expon(scale=1.0/alpha)\n",
    "# Generate n randome variables\n",
    "x = ep.rvs(size=n)\n",
    "\n",
    "s = x.sum()\n",
    "def neg_loglike(a):\n",
    "    return - n * np.log(a) + a * s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is minus the log likelihood function for the exponential distribution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimize over a reasonable parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "res = minimize_scalar(neg_loglike, bounds=(0.01, 10.0), method='bounded')\n",
    "res.x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is close to the analytical value of the max likelihood estimator we got on day 2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that a discrete Lyapunov equation is a matrix equation of the form\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    X = A X A^\\top + M\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Here all matrices are $n \\times n$ and $X$ is the unknown.  $A^\\top$ is the transpose of $A$.  The equation has a unique solution if the spectral radius of $A$ is less than 1.\n",
    "\n",
    "There is a solver for Lyapunov equations in SciPy.  Let's try it out with these matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "A = np.array([[0, 1],[-1/2, -1]])\n",
    "M = np.array([[0, 0], [0, 9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the solver and the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import solve_discrete_lyapunov\n",
    "solve_discrete_lyapunov(A, M)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact it's possible to obtain this solution by iteration, starting with a guess $X_0$, such as $X_0 = M$, and then iterating on\n",
    "\n",
    "$$\n",
    "    X_{n+1} = A X_n A^\\top + M\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to obtain the same solution using an iterative scheme.  (That is, start with $X_0$, then compute $X_1$, then $X_2$, etc.  You can stop when $X_{n+1}$ and $X_n$ are close, or by using some other simpler method.  But check that you get a result close to the solution above.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an iterative algorithm that computes the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = M\n",
    "tol = 1e-6\n",
    "max_iter = 500\n",
    "\n",
    "for i in range(max_iter):\n",
    "    P_new = A @ P @ A.T + M\n",
    "    error = np.linalg.norm(P - P_new, ord=2)\n",
    "    if error < tol:\n",
    "        break\n",
    "    P = P_new\n",
    "P"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is close to what we had before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solve_discrete_lyapunov(A, M)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "\n",
    "The task is to compute an approximation to $\\pi$ using Monte Carlo in loop.\n",
    "\n",
    "Your hints are as follows:\n",
    "\n",
    "* If $U$ is a bivariate uniform random variable on the unit square $(0, 1)^2$, then the probability that $U$ lies in a subset $B$ of $(0,1)^2$ is equal to the area of $B$.\n",
    "* If $U_1,\\ldots,U_n$ are IID copies of $U$, then, as $n$ gets large, the fraction that falls in $B$, converges to the probability of landing in $B$.\n",
    "* For a circle, $area = \\pi * radius^2$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n = 1_000_000 # sample size for Monte Carlo simulation\n",
    "\n",
    "count = 0\n",
    "for i in range(n):\n",
    "\n",
    "    # drawing random positions on the square\n",
    "    u, v = np.random.uniform(), np.random.uniform()\n",
    "\n",
    "    # check whether the point falls within the boundary\n",
    "    # of the unit circle centered at (0.5,0.5)\n",
    "    d = np.sqrt((u - 0.5)**2 + (v - 0.5)**2)\n",
    "\n",
    "    # if it falls within the inscribed circle, \n",
    "    # add it to the count\n",
    "    if d < 0.5:\n",
    "        count += 1\n",
    "\n",
    "area_estimate = count / n\n",
    "\n",
    "print(area_estimate * 4)  # dividing by radius**2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8\n",
    "\n",
    "Rewrite the Monte Carlo approximation of $\\pi$ in Exercise 7 using vectorized code. Compare the time it costs to run the vectorized code and the loop code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n = 1_000_000\n",
    "\n",
    "u, v = np.random.uniform(size=n), np.random.uniform(size=n)\n",
    "\n",
    "d = np.sqrt((u - 0.5)**2 + (v - 0.5)**2)\n",
    "\n",
    "count = np.sum(d < 0.5)\n",
    "\n",
    "area_estimate = count / n\n",
    "\n",
    "print(area_estimate * 4)  # dividing by radius**2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the vectorized version runs much faster. \n",
    "\n",
    "It enables us to do more simulations in the same amount of time to increase the precision of our simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Try to increase the sample size to 100 million\n",
    "n = 100_000_000\n",
    "\n",
    "# Drawing random positions on the square\n",
    "u, v = np.random.uniform(size=n), np.random.uniform(size=n)\n",
    "\n",
    "d = np.sqrt((u - 0.5)**2 + (v - 0.5)**2)\n",
    "\n",
    "count = np.sum(d < 0.5)\n",
    "\n",
    "area_estimate = count / n\n",
    "\n",
    "print(area_estimate * 4)  # dividing by radius**2"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
